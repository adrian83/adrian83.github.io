<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Adrian Brzoza">
    <meta name="description" content="Adrian Brzoza&#39;s personal website">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="AWS Storage Services Overview Official documentation
Back to main page
A Look at Storage Services Offered by AWS Amazon Web Services (AWS) provides low-cost data storage with high durability and availability. AWS offers storage choices for backup, archiving, and disaster recovery use cases and provides block, file, and object storage. In this whitepaper, we examine the following AWS Cloud storage services and features.
   Type Description     Amazon Simple Storage Service (Amazon S3) A service that provides scalable and highly durable object storage in the cloud."/>

    <meta property="og:title" content="" />
<meta property="og:description" content="AWS Storage Services Overview Official documentation
Back to main page
A Look at Storage Services Offered by AWS Amazon Web Services (AWS) provides low-cost data storage with high durability and availability. AWS offers storage choices for backup, archiving, and disaster recovery use cases and provides block, file, and object storage. In this whitepaper, we examine the following AWS Cloud storage services and features.
   Type Description     Amazon Simple Storage Service (Amazon S3) A service that provides scalable and highly durable object storage in the cloud." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adrian83.github.io/page/aws_architect/whitepaper_storage/" />



    
      <base href="https://adrian83.github.io/page/aws_architect/whitepaper_storage/">
    
    <title>
   · adrian
</title>

    
      <link rel="canonical" href="https://adrian83.github.io/page/aws_architect/whitepaper_storage/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.11.2/css/all.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://adrian83.github.io/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css" integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    

    <link rel="icon" type="image/png" href="https://github.githubassets.com/favicon.ico" sizes="32x32">
    <link rel="icon" type="image/png" href="https://github.githubassets.com/favicon.ico" sizes="16x16">

    <meta name="generator" content="Hugo 0.75.1" />
  </head>

  
  
  <body class="colorscheme-light">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://adrian83.github.io/">
      adrian
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://adrian83.github.io/post/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://adrian83.github.io/tags/">Tags</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://adrian83.github.io/page/about/">About</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1></h1>
    </header>

    <h1 id="aws-storage-services-overview">AWS Storage Services Overview</h1>
<p><a href="https://d0.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf">Official documentation</a></p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h2 id="a-look-at-storage-services-offered-by-aws">A Look at Storage Services Offered by AWS</h2>
<p>Amazon Web Services (AWS) provides low-cost data storage with high durability
and availability. AWS offers storage choices for backup, archiving, and disaster
recovery use cases and provides block, file, and object storage. In this whitepaper,
we examine the following AWS Cloud storage services and features.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amazon Simple Storage Service (Amazon S3)</td>
<td>A service that provides scalable and highly durable object storage in the cloud.</td>
</tr>
<tr>
<td>Amazon Glacier</td>
<td>A service that provides low-cost highly durable archive storage in the cloud.</td>
</tr>
<tr>
<td>Amazon Elastic File System (Amazon EFS)</td>
<td>A service that provides scalable network file storage for Amazon EC2 instances.</td>
</tr>
<tr>
<td>Amazon Elastic Block Store (Amazon EBS)</td>
<td>A service that provides block storage volumes for Amazon EC2 instances.</td>
</tr>
<tr>
<td>Amazon EC2 Instance Storage</td>
<td>Temporary block storage volumes for Amazon EC2 instances.</td>
</tr>
<tr>
<td>AWS Storage Gateway</td>
<td>An on-premises storage appliance that integrates with cloud storage.</td>
</tr>
<tr>
<td>AWS Snowball</td>
<td>A service that transports large amounts of data to and from the cloud.</td>
</tr>
<tr>
<td>Amazon CloudFront</td>
<td>A service that provides a global content delivery network (CDN).</td>
</tr>
</tbody>
</table>
<h3 id="s3-simple-storage-service">S3 (Simple Storage Service)</h3>
<p>Amazon Simple Storage Service (Amazon S3) provides developers and IT teams secure, durable, highly scalable object storage at a very low cost.1 You can store and retrieve any amount of data, at any time, from anywhere on the web through a simple web service interface. You can write, read, and delete objects containing from zero to 5 TB of data. Amazon S3 is highly scalable, allowing concurrent read or write access to data by many separate clients or application threads. Amazon S3 offers a range of storage classes designed for different use cases including the following:</p>
<ul>
<li>Amazon S3 Standard, for general-purpose storage of frequently accessed data</li>
<li>Amazon S3 Standard-Infrequent Access (Standard-IA), for long-lived, but less frequently accessed data</li>
<li>Amazon Glacier, for low-cost archival data</li>
</ul>
<h5 id="use-patterns">Use Patterns</h5>
<ol>
<li>Amazon S3 is used to store and distribute static web content and media.</li>
<li>Amazon S3 is used to host entire static websites. Amazon S3 provides a low-cost, highly available, and highly scalable solution, including storage for static HTML files, images, videos, and client-side scripts in formats such as JavaScript.</li>
<li>Amazon S3 is used as a data store for computation and large-scale analytics, such as financial transaction analysis, clickstream analytics, and media transcoding.</li>
<li>Amazon S3 is often used as a highly durable, scalable, and secure solution for backup and archiving of critical data.</li>
</ol>
<p>The following table presents some storage needs for which you should consider other AWS storage options.</p>
<table>
<thead>
<tr>
<th>Storage Need</th>
<th>Solution</th>
<th>AWS Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>File system</td>
<td>Amazon S3 uses a flat namespace and isn’t meant to serve as a standalone, POSIX-compliant file system. Instead, consider using Amazon EFS as a file system.</td>
<td><a href="http://aws.amazon.com/efs/">EFS</a></td>
</tr>
<tr>
<td>Structured data with query</td>
<td>Amazon S3 doesn’t offer query capabilities to retrieve specific objects. When you use Amazon S3 you need to know the exact bucket name and key for the files you want to retrieve from the service. Amazon S3 can’t be used as a database or search engine by itself. Instead, you can pair Amazon S3 with Amazon DynamoDB, Amazon CloudSearch, or Amazon Relational Database Service (Amazon RDS) to index and query metadata about Amazon S3 buckets and objects.</td>
<td><a href="http://aws.amazon.com/dynamodb/">DynamoDB</a>, <a href="http://aws.amazon.com/rds/">RDS</a>, <a href="http://aws.amazon.com/cloudsearch/">CloudSearch</a></td>
</tr>
<tr>
<td>Rapidly changing data</td>
<td>Data that must be updated very frequently might be better served by storage solutions that take into account read and write latencies, such as Amazon EBS volumes, Amazon RDS, Amazon DynamoDB, Amazon EFS, or relational databases running on Amazon EC2.</td>
<td><a href="https://aws.amazon.com/ebs/">EBS</a>, <a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/dynamodb/">DynamoDB</a>, <a href="http://aws.amazon.com/rds/">RDS</a></td>
</tr>
<tr>
<td>Archival data</td>
<td>Data that requires encrypted archival storage with infrequent read access with a long recovery time objective (RTO) can be stored in Amazon Glacier more cost-effectively.</td>
<td><a href="http://aws.amazon.com/glacier/">Glacier</a></td>
</tr>
<tr>
<td>Dynamic website hosting</td>
<td>Although Amazon S3 is ideal for static content websites, dynamic websites that depend on database interaction or use server-side scripting should be hosted on Amazon EC2 or Amazon EFS.</td>
<td><a href="http://aws.amazon.com/ec2/">EC2</a>, <a href="http://aws.amazon.com/efs/">EFS</a></td>
</tr>
<tr>
<td>Rapidly changing data</td>
<td>Data that must be updated very frequently might be better served by a storage solution with lower read/write latencies, such as Amazon EBS, Amazon RDS, Amazon EFS, Amazon DynamoDB, or relational databases running on Amazon EC2.</td>
<td><a href="https://aws.amazon.com/ebs/">EBS</a>, <a href="http://aws.amazon.com/rds/">RDS</a>, <a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/dynamodb/">DynamoDB</a>, <a href="http://aws.amazon.com/ec2/">EC2</a></td>
</tr>
<tr>
<td>Immediate access</td>
<td>Data stored in Amazon Glacier is not available immediately. Retrieval jobs typically require 3–5 hours to complete, so if you need immediate access to your object data, Amazon S3 is a better choice</td>
<td><a href="http://aws.amazon.com/s3/">S3</a></td>
</tr>
</tbody>
</table>
<h5 id="performence">Performence</h5>
<p>In scenarios where you use Amazon S3 from the same Region, access to Amazon S3 from Amazon EC2 is designed to be fast.</p>
<p>If you access Amazon S3 using multiple threads, multiple applications, or multiple clients concurrently, total Amazon S3 aggregate throughput typically scales to rates that far exceed what any single server can generate or consume.</p>
<p>To improve the upload performance of large objects (typically over 100 MB), Amazon S3 offers a multipart upload command to upload a single object as a set of parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. Using multipart upload, you can get improved throughput and quick recovery from any network issues. Another benefit of using multipart upload is that you can upload multiple parts of a single object in parallel and restart the upload of smaller parts instead of restarting the upload of the entire large object.</p>
<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfer of files over long distances between your client and your Amazon S3 bucket. It leverages Amazon CloudFront globally distributed edge locations to route traffic to your Amazon S3 bucket over an Amazon-optimized network path. To get started with Amazon S3 Transfer Acceleration you first must enable it on an Amazon S3 bucket. Then modify your Amazon S3 PUT and GET requests to use the s3-accelerate endpoint domain name (&lt; bucketname &gt;.s3-accelerate.amazonaws.com). The Amazon S3 bucket can still be accessed using the regular endpoint. Some customers have measured performance improvements in excess of 500 percent when performing intercontinental uploads.</p>
<h5 id="durability-and-availability">Durability and Availability</h5>
<p>Amazon S3 Standard storage and Standard-IA storage provide high levels of data durability and availability by automatically and synchronously storing your data across both multiple devices and multiple facilities within your selected geographical region. Error correction is built-in, and there are no single points of failure. Amazon S3 is designed to sustain the concurrent loss of data in two facilities, making it very well suited to serve as the primary data storage for mission-critical data. In fact, Amazon S3 is designed for 99.999999999 percent (11 nines) durability per object and 99.99 percent availability over a one-year period.</p>
<p>Additionally, you have a choice of enabling cross-region replication on each Amazon S3 bucket. Once enabled, cross-region replication automatically copies objects across buckets in different AWS Regions asynchronously, providing 11 nines of durability and 4 nines of availability on both the source and destination Amazon S3 objects.</p>
<h5 id="security">Security</h5>
<p>You can manage access to Amazon S3 by granting other AWS accounts and users permission to perform the resource operations by writing an access policy.</p>
<p>You can protect Amazon S3 data at rest by using server-side encryption, in which you request Amazon S3 to encrypt your object before it’s written to disks in data centers and decrypt it when you download the object or by using client-side encryption, in which you encrypt your data on the client side and upload the encrypted data to Amazon S3. You can protect the data in transit by using Secure Sockets Layer (SSL) or client-side encryption.</p>
<p>You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. Additionally, you can add an optional layer of security by enabling Multi-Factor Authentication (MFA) Delete for a bucket.7 With this option enabled for a bucket, two forms of authentication are required to change the versioning state of the bucket or to permanently delete an object version: valid AWS account credentials plus a sixdigit code (a single-use, time-based password) from a physical or virtual token device.</p>
<p>To track requests for access to your bucket, you can enable access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any. Access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>
<h5 id="interfaces">Interfaces</h5>
<p>Amazon S3 provides:</p>
<ol>
<li>Standards-based REST web service application program interfaces (APIs) for both management and data operations.</li>
<li>Higher-level toolkit or software development kit (SDK) that wraps the underlying REST API.</li>
<li>Integrated AWS Command Line Interface (AWS CLI) also provides a set of high-level, Linux-like Amazon S3 file commands for common operations, such as ls, cp, mv, sync, and so on</li>
</ol>
<p>Additionally, you can use the Amazon S3 notification feature to receive notifications when certain events happen in your bucket. Currently, Amazon S3 can publish events when an object is uploaded or when an object is deleted.</p>
<p>Notifications can be issued to Amazon Simple Notification Service (SNS) topics, Amazon Simple Queue Service (SQS) queues, and AWS Lambda functions.</p>
<h5 id="cost-model">Cost Model</h5>
<p>With Amazon S3, you pay only for the storage you actually use. Amazon S3 Standard has three pricing components: storage (per GB per month), data transfer in or out (per GB per month), and requests (per thousand requests per month). There are Data Transfer IN and OUT fees if you enable Amazon S3 Transfer Acceleration on a bucket and the transfer performance is faster than regular Amazon S3 transfer.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="amazon-glacier">Amazon Glacier</h3>
<p>Amazon Glacier is an extremely low-cost storage service that provides highly secure, durable, and flexible storage for data archiving and online backup.14 With Amazon Glacier, you can reliably store your data for as little as $0.007 per gigabyte per month. Amazon Glacier enables you to offload the administrative burdens of operating and scaling storage to AWS so that you don’t have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and repair, or time-consuming hardware migrations.</p>
<p>You store data in Amazon Glacier as archives. An archive can represent a single file, or you can combine several files to be uploaded as a single archive.</p>
<p>Retrieving archives from Amazon Glacier requires the initiation of a job. You organize your archives in vaults.</p>
<p>Amazon Glacier is designed for use with other Amazon web services. You can seamlessly move data between Amazon Glacier and Amazon S3 using S3 data lifecycle policies.</p>
<h5 id="use-patterns-1">Use Patterns</h5>
<p>Organizations are using Amazon Glacier to support a number of use cases. These use cases include archiving offsite enterprise information, media assets, and research and scientific data, and also performing digital preservation and magnetic tape replacement.</p>
<p>Amazon Glacier doesn’t suit all storage situations. The following table presents a few storage needs for which you should consider other AWS storage options.</p>
<table>
<thead>
<tr>
<th>Storage Need</th>
<th>Solution</th>
<th>AWS Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rapidly changing data</td>
<td>Data that must be updated very frequently might be better served by a storage solution with lower read/write latencies, such as Amazon EBS, Amazon RDS, Amazon EFS, Amazon DynamoDB, or relational databases running on Amazon EC2.</td>
<td><a href="https://aws.amazon.com/ebs/">EBS</a>, <a href="http://aws.amazon.com/rds/">RDS</a>, <a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/dynamodb/">DynamoDB</a>, <a href="http://aws.amazon.com/ec2/">EC2</a></td>
</tr>
<tr>
<td>Immediate access</td>
<td>Data stored in Amazon Glacier is not available immediately. Retrieval jobs typically require 3–5 hours to complete, so if you need immediate access to your object data, Amazon S3 is a better choice</td>
<td><a href="http://aws.amazon.com/s3/">S3</a></td>
</tr>
</tbody>
</table>
<h5 id="performance">Performance</h5>
<p>Amazon Glacier is a low-cost storage service designed to store data that is infrequently accessed and long-lived. Amazon Glacier retrieval jobs typically complete in 3 to 5 hours.</p>
<p>You can improve the upload experience for larger archives by using multipart upload for archives up to about 40 TB (the single archive limit). You can upload separate parts of a large archive independently, in any order and in parallel, to improve the upload experience for larger archives. You can even perform range retrievals on archives stored in Amazon Glacier by specifying a range or portion of the archive. Specifying a range of bytes for a retrieval can help control bandwidth costs, manage your data downloads, and retrieve a targeted part of a large archive.</p>
<h5 id="durability-and-availability-1">Durability and Availability</h5>
<p>Amazon Glacier is designed to provide average annual durability of 99.999999999 percent (11 nines) for an archive. The service redundantly stores data in multiple facilities and on multiple devices within each facility. To increase durability, Amazon Glacier synchronously stores your data across multiple facilities before returning SUCCESS on uploading an archive.</p>
<h5 id="scalability-and-elasticity">Scalability and Elasticity</h5>
<p>Amazon Glacier scales to meet growing and often unpredictable storage requirements. A single archive is limited to 40 TB in size, but there is no limit to the total amount of data you can store in the service.</p>
<h5 id="security-1">Security</h5>
<p>By default, only you can access your Amazon Glacier data. If other people need to access your data, you can set up data access control in Amazon Glacier by using the AWS Identity and Access Management (IAM) service. To do so, simply create an IAM policy that specifies which account users have rights to operations on a given vault.</p>
<p>Amazon Glacier uses server-side encryption to encrypt all data at rest. Amazon Glacier handles key management and key protection for you by using one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES256). Customers who want to manage their own keys can encrypt data prior to uploading it.</p>
<p>Amazon Glacier allows you to lock vaults where long-term records retention is mandated by regulations or compliance rules. You can set compliance controls on individual Amazon Glacier vaults and enforce these by using lockable policies.</p>
<p>To help monitor data access, Amazon Glacier is integrated with AWS CloudTrail, allowing any API calls made to Amazon Glacier in your AWS account to be captured and stored in log files that are delivered to an Amazon S3 bucket that you specify.</p>
<h5 id="interfaces-1">Interfaces</h5>
<p>Amazon Galcier provides:</p>
<ol>
<li>Standards-based REST web service application program interfaces (APIs) for both management and data operations.</li>
<li>Higher-level toolkit or software development kit (SDK) that wraps the underlying REST API.</li>
<li>Integrated AWS Command Line Interface (AWS CLI)</li>
</ol>
<p>Second, Amazon Glacier can be used as a storage class in Amazon S3 by using object lifecycle management that provides automatic, policy-driven archiving from Amazon S3 to Amazon Glacier. You simply set one or more lifecycle rules for an Amazon S3 bucket, defining what objects should be transitioned to Amazon Glacier and when. You can specify an absolute or relative time period (including 0 days) after which the specified Amazon S3 objects should be transitioned to Amazon Glacier. The Amazon S3 API includes a RESTORE operation. The retrieval process from Amazon Glacier using RESTORE takes three to five hours, the same as other Amazon Glacier retrievals.</p>
<h5 id="cost-model-1">Cost Model</h5>
<p>With Amazon Glacier, you pay only for what you use and there is no minimum fee. In normal use, Amazon Glacier has three pricing components: storage (per GB per month), data transfer out (per GB per month), and requests (per thousand UPLOAD and RETRIEVAL requests per month).</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="amazon-efs-elastic-file-system">Amazon EFS (Elastic File System)</h3>
<p>Amazon Elastic File System (Amazon EFS) delivers a simple, scalable, elastic, highly available, and highly durable network file system as a service to EC2 instances.
Amazon EFS is designed to provide a highly scalable network file system that can grow to petabytes, which allows massively parallel access from EC2 instances to your data within a Region. It is also highly available and highly durable because it stores data and metadata across multiple Availability Zones in a Region.</p>
<p>To understand Amazon EFS, it is best to examine the different components that allow EC2 instances access to EFS file systems. You can create one or more EFS file systems within an AWS Region. Each file system is accessed by EC2 instances via mount targets, which are created per Availability Zone. You create one mount target per Availability Zone in the VPC you create using Amazon Virtual Private Cloud. Traffic flow between Amazon EFS and EC2 instances is controlled using security groups associated with the EC2 instance and the EFS mount targets. Access to EFS file system objects (files and directories) is controlled using standard Unix-style read/write/execute permissions based on user and group IDs</p>
<h5 id="usage-patterns">Usage Patterns</h5>
<p>Amazon EFS is designed to meet the needs of multi-threaded applications and applications that concurrently access data from multiple EC2 instances and that require substantial levels of aggregate throughput and input/output operations per second (IOPS). Its distributed design enables high levels of availability, durability, and scalability, which results in a small latency overhead for each file operation. Because of this per-operation overhead, overall throughput generally increases as the average input/output (I/O) size increases since the overhead is amortized over a larger amount of data. This makes Amazon EFS ideal for growing datasets consisting of larger files that need both high performance and multi-client access.</p>
<p>Amazon EFS supports highly parallelized workloads and is designed to meet the performance needs of big data and analytics, media processing, content management, web serving, and home directories.</p>
<p>Amazon EFS doesn’t suit all storage situations. The following table presents some storage needs for which you should consider other AWS storage options.</p>
<table>
<thead>
<tr>
<th>Storage Need</th>
<th>Solution</th>
<th>AWS Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>Archival data</td>
<td>Data that requires encrypted archival storage with infrequent read access with a long recovery time objective (RTO) can be stored in Amazon Glacier more cost-effectively.</td>
<td><a href="http://aws.amazon.com/glacier/">Glacier</a></td>
</tr>
<tr>
<td>Relational database storage</td>
<td>In most cases, relational databases require storage that is mounted, accessed, and locked by a single node (EC2 instance, etc.). When running relational databases on AWS, look at leveraging Amazon RDS or Amazon EC2 with Amazon EBS PIOPS volumes.</td>
<td><a href="https://aws.amazon.com/ebs/">EBS</a>, <a href="http://aws.amazon.com/rds/">RDS</a>, <a href="http://aws.amazon.com/ec2/">EC2</a></td>
</tr>
<tr>
<td>Temporary storage</td>
<td>Consider using local instance store volumes for needs such as scratch disks, buffers, queues, and caches.</td>
<td><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">EC2 Local Instance Store</a></td>
</tr>
</tbody>
</table>
<h5 id="performance-1">Performance</h5>
<p>Amazon EFS file systems are distributed across an unconstrained number of storage servers, enabling file systems to grow elastically to petabyte-scale and allowing massively parallel access from EC2 instances within a Region. This distributed data storage design means that multi-threaded applications and applications that concurrently access data from multiple EC2 instances can drive substantial levels of aggregate throughput and IOPS.</p>
<p>There are two different performance modes available for Amazon EFS: General Purpose and Max I/O. General Purpose performance mode is the default mode and is appropriate for most file systems. However, if your overall Amazon EFS workload will exceed 7,000 file operations per second per file system, we recommend the files system use Max I/O performance mode. Max I/O performance mode is optimized for applications where tens, hundreds, or thousands of EC2 instances are accessing the file system. With this mode, file systems scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations.</p>
<p>Due to the spiky nature of file-based workloads, Amazon EFS is optimized to burst at high-throughput levels for short periods of time, while delivering low levels of throughput the rest of the time. A credit system determines when an Amazon EFS file system can burst. Over time, each file system earns burst credits at a baseline rate, determined by the size of the file system, and uses these credits whenever it reads or writes data. A file system can drive throughput continuously at its baseline rate. It accumulates credits during periods of inactivity or when throughput is below its baseline rate. These accumulated burst credits allow a file system to drive throughput above its baseline rate. The file system can continue to drive throughput above its baseline rate as long as it has a positive burst credit balance. You can see the burst credit balance for a file system by viewing the BurstCreditBalance metric in Amazon CloudWatch.</p>
<h5 id="durability-and-availability-2">Durability and Availability</h5>
<p>Amazon EFS is designed to be highly durable and highly available. Each Amazon EFS file system object (such as a directory, file, or link) is redundantly stored across multiple Availability Zones within a Region. Amazon EFS is designed to be as highly durable and available as Amazon S3.</p>
<h5 id="scalability-and-elasticity-1">Scalability and Elasticity</h5>
<p>Amazon EFS automatically scales your file system storage capacity up or down as you add or remove files without disrupting your applications, giving you just the storage you need, when you need it, and while eliminating time-consuming administration tasks associated with traditional storage management (such as planning, buying, provisioning, and monitoring). Your EFS file system can grow from an empty file system to multiple petabytes automatically, and there is no provisioning, allocating, or administration.</p>
<h5 id="security-2">Security</h5>
<p>There are three levels of access control to consider when planning your EFS file system security:</p>
<ol>
<li>IAM permissions for API calls</li>
<li>Security groups for EC2 instances and mount targets</li>
<li>and Network File System-level users, groups, and permissions</li>
</ol>
<h5 id="interfaces-2">Interfaces</h5>
<p>Amazon offers a network protocol-based HTTP (RFC 2616) API for managing Amazon EFS, as well as supporting for EFS operations within the AWS SDKs and the AWS CLI. The API actions and EFS operations are used to create, delete, and describe file systems; create, delete, and describe mount targets; create, delete, and describe tags; and describe and modify mount target security groups.</p>
<h5 id="cost-model-2">Cost Model</h5>
<p>This highly durable, highly available architecture is built into the pricing model, and you only pay for the amount of storage you put into your file system. As files are added, your EFS file system dynamically grows, and you only pay for the amount of storage you use. As files are removed, your EFS file system dynamically shrinks, and you stop paying for the data you deleted. There are no charges for bandwidth or requests, and there are no minimum commitments or up-front fees.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="amazon-ebs-elastic-block-store">Amazon EBS (Elastic Block Store)</h3>
<p>Amazon Elastic Block Store (Amazon EBS) volumes provide durable block-level storage for use with EC2 instances.28 Amazon EBS volumes are network-attached storage that persists independently from the running life of a single EC2 instance. After an EBS volume is attached to an EC2 instance, you can use the EBS volume like a physical hard drive, typically by formatting it with the file system of your choice and using the file I/O interface provided by the instance operating system. Most Amazon Machine Images (AMIs) are backed by Amazon EBS, and use an EBS volume to boot EC2 instances. You can also attach multiple EBS volumes to a single EC2 instance. Note, however, that any single EBS volume can be attached to only one EC2 instance at any time.</p>
<p>EBS also provides the ability to create point-in-time snapshots of volumes, which are stored in Amazon S3. These snapshots can be used as the starting point for new EBS volumes and to protect data for long-term durability. The same snapshot can be used to instantiate as many volumes as you want. These snapshots can be copied across AWS Regions, making it easier to leverage multiple AWS Regions for geographical expansion, data center migration, and disaster recovery. Sizes for EBS volumes range from 1 GiB to 16 TiB, depending on the volume type, and are allocated in 1 GiB increments.</p>
<h5 id="usage-patterns-1">Usage Patterns</h5>
<p>Amazon EBS is meant for data that changes relatively frequently and needs to persist beyond the life of EC2 instance. Amazon EBS is well-suited for use as the primary storage for a database or file system, or for any application or instance (operating system) that requires direct access to raw block-level storage. Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories:</p>
<ol>
<li>Solid-state drive (SSD)-backed storage for transactional workloads such as databases and boot volumes (performance depends primarily on IOPS)
<ul>
<li>SSD-Backed Provisioned IOPS (io1)
<ul>
<li>I/O-intensive NoSQL and relational databases</li>
<li>Volume Size: 4 GiB – 16 TiB</li>
<li>Max IOPS per Volume: 20,000</li>
<li>Max Throughput per Volume: 320 MiB/s</li>
<li>Max IOPS per Instance: 65,000</li>
<li>Max Throughput per Instance: 1,250 MiB/s</li>
<li>Dominant Performance Attribute: IOPS</li>
</ul>
</li>
<li>SSD-Backed General Purpose (gp2)
<ul>
<li>Boot volumes, low-latency interactive apps, dev &amp; test</li>
<li>Volume Size: 1 GiB – 16 TiB</li>
<li>Max IOPS per Volume: 10,000</li>
<li>Max Throughput per Volume: 160 MiB/s</li>
<li>Max IOPS per Instance: 65,000</li>
<li>Max Throughput per Instance: 1,250 MiB/s</li>
<li>Dominant Performance Attribute: IOPS</li>
</ul>
</li>
</ul>
</li>
<li>Hard disk drive (HDD)-backed storage for throughput-intensive workloads such as big data, data warehouse, and log processing (performance depends primarily on MB/s).
<ul>
<li>HDD-Backed Throughput Optimized (st1)
<ul>
<li>Big data, data warehouse, log processing</li>
<li>Volume Size: 500 GiB – 16 TiB</li>
<li>Max IOPS per Volume: 500</li>
<li>Max Throughput per Volume: 500 MiB/s</li>
<li>Max IOPS per Instance: 65,000</li>
<li>Max Throughput per Instance: 1,250 MiB/s</li>
<li>Dominant Performance Attribute: MiB/s</li>
</ul>
</li>
<li>HDD-Backed Cold (sc1)
<ul>
<li>Colder data requiring fewer scans per day</li>
<li>Volume Size: 500 GiB – 16 TiB</li>
<li>Max IOPS per Volume: 250</li>
<li>Max Throughput per Volume: 250 MiB/s</li>
<li>Max IOPS per Instance: 65,000</li>
<li>Max Throughput per Instance: 1,250 MiB/s</li>
<li>Dominant Performance Attribute: MiB/s</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>Amazon EBS doesn’t suit all storage situations. The following table presents some storage needs for which you should consider other AWS storage options.</p>
<table>
<thead>
<tr>
<th>Storage Need</th>
<th>Solution</th>
<th>AWS Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>Temporary storage</td>
<td>Consider using local instance store volumes for needs such as scratch disks, buffers, queues, and caches.</td>
<td><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">EC2 Local Instance Store</a></td>
</tr>
<tr>
<td>Multi-instance storage</td>
<td>Amazon EBS volumes can only be attached to one EC2 instance at a time. If you need multiple EC2 instances accessing volume data at the same time, consider using Amazon EFS as a file system.</td>
<td><a href="http://aws.amazon.com/efs/">EFS</a></td>
</tr>
<tr>
<td>Highly durable storage</td>
<td>If you need very highly durable storage, use S3 or Amazon EFS. Amazon S3 Standard storage is designed for 99.999999999 percent (11 nines) annual durability per object. You can even decide to take a snapshot of the EBS volumes. Such a snapshot then gets saved in Amazon S3, thus providing you the durability of Amazon S3. For more information on EBS durability, see the Durability and Availability section. EFS is designed for high durability and high availability, with data stored in multiple Availability Zones within an AWS Region.</td>
<td><a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/s3/">S3</a></td>
</tr>
<tr>
<td>Static data or web content</td>
<td>If your data doesn’t change that often, Amazon S3 might represent a more cost-effective and scalable solution for storing this fixed information. Also, web content served out of Amazon EBS requires a web server running on Amazon EC2; in contrast, you can deliver web content directly out of Amazon S3 or from multiple EC2 instances using Amazon EFS.</td>
<td><a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/s3/">S3</a></td>
</tr>
</tbody>
</table>
<h5 id="performance-2">Performance</h5>
<p>As described previously, Amazon EBS provides a range of volume types that are divided into two major categories: SSD-backed storage volumes and HDD-backed storage volumes. SSD-backed storage volumes offer great price/performance characteristics for random small block workloads, such as transactional applications, whereas HDD-backed storage volumes offer the best price/performance characteristics for large block sequential workloads. You can attach and stripe data across multiple volumes of any type to increase the I/O performance available to your Amazon EC2 applications. The following table presents the storage characteristics of the current generation volume types.</p>
<p>Because all EBS volumes are network-attached devices, other network I/O performed by an EC2 instance, as well as the total load on the shared network, can affect the performance of individual EBS volumes. To enable your EC2 instances to maximize the performance of EBS volumes, you can launch selected EC2 instance types as EBS-optimized instances. Most of the latest generation EC2 instances are EBS-optimized by default. EBS-optimized instances deliver dedicated throughput between Amazon EC2 and Amazon EBS, with speeds between 500 Mbps and 10,000 Mbps depending on the instance type. When attached to EBS-optimized instances, provisioned IOPS volumes are designed to deliver within 10 percent of the provisioned IOPS performance 99.9 percent of the time within a given year. Newly created EBS volumes receive their maximum performance the moment they are available, and they don’t require initialization (formerly known as prewarming). However, you must initialize the storage blocks on volumes that were restored from snapshots before you can access the block.</p>
<h5 id="durability-and-availability-3">Durability and Availability</h5>
<p>Amazon EBS volumes are designed to be highly available and reliable. EBS volume data is replicated across multiple servers in a single Availability Zone to prevent the loss of data from the failure of any single component. Taking snapshots of your EBS volumes increases the durability of the data stored on your EBS volumes. EBS snapshots are incremental, point-in-time backups, containing only the data blocks changed since the last snapshot. EBS volumes are designed for an annual failure rate (AFR) of between 0.1 and 0.2 percent, where failure refers to a complete or partial loss of the volume, depending on the size and performance of the volume. This means, if you have 1,000 EBS volumes over the course of a year, you can expect unrecoverable failures with 1 or 2 of your volumes. This AFR makes EBS volumes 20 times more reliable than typical commodity disk drives, which fail with an AFR of around 4 percent. Despite these very low EBS AFR numbers, we still recommend that you create snapshots of your EBS volumes to improve the durability of your data. The Amazon EBS snapshot feature makes it easy to take application-consistent backups of your data.</p>
<h5 id="scalability-and-elasticity-2">Scalability and Elasticity</h5>
<p>Using the AWS Management Console or the Amazon EBS API, you can easily and rapidly provision and release EBS volumes to scale in and out with your total storage demands.</p>
<p>The simplest approach is to create and attach a new EBS volume and begin using it together with your existing ones. However, if you need to expand the size of a single EBS volume, you can effectively resize a volume using a snapshot:</p>
<ol>
<li>Detach the original EBS volume.</li>
<li>Create a snapshot of the original EBS volume’s data in Amazon S3.</li>
<li>Create a new EBS volume from the snapshot but specify a larger size than the original volume.</li>
<li>Attach the new, larger volume to your EC2 instance in place of the original. (In many cases, an OS-level utility must also be used to expand the file system.)</li>
<li>Delete the original EBS volume.</li>
</ol>
<h5 id="security-3">Security</h5>
<p>IAM enables access control for your EBS volumes, allowing you to specify who can access which EBS volumes.</p>
<p>EBS encryption enables data-at-rest and data-in-motion security. It offers seamless encryption of both EBS boot volumes and data volumes as well as snapshots, eliminating the need to build and manage a secure key management infrastructure. These encryption keys are Amazon-managed or keys that you create and manage using the AWS Key Management Service (AWS KMS). Datain-motion security occurs on the servers that host EC2 instances, providing encryption of data as it moves between EC2 instances and EBS volumes.</p>
<h5 id="interfaces-3">Interfaces</h5>
<p>Amazon offers a REST management API for Amazon EBS, as well as support for Amazon EBS operations within both the AWS SDKs and the AWS CLI.</p>
<h5 id="cost-model-3">Cost Model</h5>
<p>As with other AWS services, with Amazon EBS you pay only for what you provision, in increments down to 1 GB. In contrast, hard disks come in fixed sizes and you pay for the entire size of the disk regardless of the amount you use or allocate. Amazon EBS pricing has three components: provisioned storage, I/O requests, and snapshot storage. Amazon EBS General Purpose (SSD), Throughput Optimized (HDD), and Cold (HDD) volumes are charged per GBmonth of provisioned storage. Amazon EBS Provisioned IOPS (SSD) volumes are charged per GB-month of provisioned storage and per provisioned IOPS-month. For all volume types, Amazon EBS snapshots are charged per GB-month of data stored. An Amazon EBS snapshot copy is charged for the data transferred between Regions and for the standard Amazon EBS snapshot charges in the destination Region.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="amazon-ec2-instance-storage">Amazon EC2 Instance Storage</h3>
<p>Amazon EC2 instance store volumes (also called ephemeral drives) provide temporary block-level storage for many EC2 instance types. This storage consists of a preconfigured and pre-attached block of disk storage on the same physical server that hosts the EC2 instance for which the block provides storage. The amount of the disk storage provided varies by EC2 instance type. In the EC2 instance families that provide instance storage, larger instances tend to provide both more and larger instance store volumes.</p>
<p>AWS offers two EC2 instance families that are purposely built for storage-centric
workloads:</p>
<ol>
<li>SSD-Backed Storage-optimized (i2) - NoSQL databases, like Cassandra and MongoDB, scale out transactional databases, data warehousing, Hadoop, and cluster file systems.</li>
<li>HDD-Backed Dense-storage (d2) - Massively Parallel Processing (MPP) data warehousing, MapReduce and Hadoop distributed computing, distributed file systems, network file systems, log or data-processing applications</li>
</ol>
<h5 id="usage-patterns-2">Usage Patterns</h5>
<p>In general, EC2 local instance store volumes are ideal for temporary storage of information that is continually changing, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. EC2 instance storage is well-suited for this purpose. It consists of the virtual machine’s boot device (for instance store AMIs only), plus one or more additional volumes that are dedicated to the EC2 instance (for both Amazon EBS AMIs and instance store AMIs). This storage can only be used from a single EC2 instance during that instance&rsquo;s lifetime. Note that, unlike EBS volumes, instance store volumes cannot be detached or attached to another instance.</p>
<p>EC2 instance store volumes don’t suit all storage situations. The following table presents some storage needs for which you should consider other AWS storage options.</p>
<table>
<thead>
<tr>
<th>Storage Need</th>
<th>Solution</th>
<th>AWS Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>Persistent storage</td>
<td>If you need persistent virtual disk storage similar to a physical disk drive for files or other data that must persist longer than the lifetime of a single EC2 instance, EBS volumes, Amazon EFS file systems, or Amazon S3 are more appropriate.</td>
<td><a href="http://aws.amazon.com/ec2/">EC2</a>, <a href="https://aws.amazon.com/ebs/">EBS</a>, <a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/s3/">S3</a></td>
</tr>
<tr>
<td>Relational database storage</td>
<td>In most cases, relational databases require storage that persists beyond the lifetime of a single EC2 instance, making EBS volumes the natural choice.</td>
<td><a href="http://aws.amazon.com/ec2/">EC2</a>, <a href="https://aws.amazon.com/ebs/">EBS</a></td>
</tr>
<tr>
<td>Shared storage</td>
<td>Instance store volumes are dedicated to a single EC2 instance and can’t be shared with other systems or users. If you need storage that can be detached from one instance and attached to a different instance, or if you need the ability to share data easily, Amazon EFS, Amazon S3, or Amazon EBS are better choices.</td>
<td><a href="http://aws.amazon.com/efs/">EFS</a>, <a href="http://aws.amazon.com/s3/">S3</a>, <a href="https://aws.amazon.com/ebs/">EBS</a></td>
</tr>
<tr>
<td>Snapshots</td>
<td>If you need the convenience, long-term durability, availability, and the ability to share point-in-time disk snapshots, EBS volumes are a better choice.</td>
<td><a href="https://aws.amazon.com/ebs/">EBS</a></td>
</tr>
</tbody>
</table>
<h5 id="performance-3">Performance</h5>
<p>The instance store volumes that are not SSD-based in most EC2 instance families have performance characteristics similar to standard EBS volumes. Because the EC2 instance virtual machine and the local instance store volumes are located on the same physical server, interaction with this storage is very fast, particularly for sequential access. To increase aggregate IOPS, or to improve sequential disk throughput, multiple instance store volumes can be grouped together using RAID 0 (disk striping) software. Because the bandwidth of the disks is not limited by the network, aggregate sequential throughput for multiple instance volumes can be higher than for the same number of EBS volumes.</p>
<p>If you require high disk performance, we recommend that you prewarm your drives by writing once to every drive location before production use. The i2, r3, and hi1 instance types use direct-attached SSD backing that provides maximum performance at launch time without prewarming.</p>
<h5 id="durability-and-availability-4">Durability and Availability</h5>
<p>Amazon EC2 local instance store volumes are not intended to be used as durable disk storage. Unlike Amazon EBS volume data, data on instance store volumes persists only during the life of the associated EC2 instance.</p>
<h5 id="scalability-and-elasticity-3">Scalability and Elasticity</h5>
<p>The number and storage capacity of Amazon EC2 local instance store volumes are fixed and defined by the instance type.</p>
<h5 id="security-4">Security</h5>
<p>IAM helps you securely control which users can perform operations such as launch and termination of EC2 instances in your account, and instance store volumes can only be mounted and accessed by the EC2 instances they belong to. Also, when you stop or terminate an instance, the applications and data in its instance store are erased, so no other instance can have access to the instance store in the future.</p>
<h5 id="interfaces-4">Interfaces</h5>
<p>There is no separate management API for EC2 instance store volumes. Instead, instance store volumes are specified using the block device mapping feature of the Amazon EC2 API and the AWS Management Console. You cannot create or destroy instance store volumes, but you can control whether or not they are exposed to the EC2 instance and what device name is mapped to for each volume.</p>
<h5 id="cost-model-4">Cost Model</h5>
<p>The cost of an EC2 instance includes any local instance store volumes, if the instance type provides them.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="aws-storage-gateway">AWS Storage Gateway</h3>
<p>AWS Storage Gateway connects an on-premises software appliance with cloudbased storage to provide seamless and secure storage integration between an organization’s on-premises IT environment and the AWS storage infrastructure. The service enables you to securely store data in the AWS Cloud for scalable and cost-effective storage. AWS Storage Gateway supports industrystandard storage protocols that work with your existing applications. It provides low-latency performance by maintaining frequently accessed data on-premises while securely storing all of your data encrypted in Amazon S3 or Amazon Glacier. For disaster recovery scenarios, AWS Storage Gateway, together with Amazon EC2, can serve as a cloud-hosted solution that mirrors your entire production environment.</p>
<p>You can download the AWS Storage Gateway software appliance as a virtual machine (VM) image that you install on a host in your data center or as an EC2 instance. Once you’ve installed your gateway and associated it with your AWS account through the AWS activation process, you can use the AWS Management Console to create gateway-cached volumes, gateway-stored volumes, or a gateway-virtual tape library (VTL), each of which can be mounted as an iSCSI device by your on-premises applications.</p>
<p>With gateway-cached volumes, you can use Amazon S3 to hold your primary data, while retaining some portion of it locally in a cache for frequently accessed data. Gateway-cached volumes minimize the need to scale your on-premises storage infrastructure while still providing your applications with low-latency access to their frequently accessed data. You can create storage volumes up to 32 TiB in size and mount them as iSCSI devices from your on-premises application servers. Each gateway configured for gateway-cached volumes can support up to 20 volumes and total volume storage of 150 TiB. Data written to these volumes is stored in Amazon S3, with only a cache of recently written and recently read data stored locally on your on-premises storage hardware.</p>
<p>Gateway-stored volumes store your primary data locally, while asynchronously backing up that data to AWS. These volumes provide your on-premises applications with low-latency access to their entire datasets, while providing durable, off-site backups. You can create storage volumes up to 1 TiB in size and mount them as iSCSI devices from your on-premises application servers. Each gateway configured for gateway-stored volumes can support up to 12 volumes and total volume storage of 12 TiB. Data written to your gateway-stored volumes is stored on your on-premises storage hardware, and asynchronously backed up to Amazon S3 in the form of Amazon EBS snapshots.</p>
<p>A gateway-VTL allows you to perform offline data archiving by presenting your existing backup application with an iSCSI-based virtual tape library consisting of a virtual media changer and virtual tape drives. You can create virtual tapes in your VTL by using the AWS Management Console, and you can size each virtual tape from 100 GiB to 2.5 TiB. A VTL can hold up to 1,500 virtual tapes, with a maximum aggregate capacity of 150 TiB. Once the virtual tapes are created, your backup application can discover them by using its standard media inventory procedure. Once created, tapes are available for immediate access and are stored in Amazon S3.</p>
<p>Virtual tapes that you need to access frequently should be stored in a VTL. Data that you don&rsquo;t need to retrieve frequently can be archived to your virtual tape shelf (VTS), which is stored in Amazon Glacier, further reducing your storage costs.</p>
<h5 id="usage-patterns-3">Usage Patterns</h5>
<p>Organizations are using AWS Storage Gateway to support a number of use cases. These use cases include corporate file sharing, enabling existing on-premises backup applications to store primary backups on Amazon S3, disaster recovery, and mirroring data to cloud-based compute resources and then later archiving it to Amazon Glacier.</p>
<h5 id="performance-4">Performance</h5>
<p>Because the AWS Storage Gateway VM sits between your application, Amazon S3, and underlying on-premises storage, the performance you experience depends upon a number of factors. These factors include the speed and configuration of your underlying local disks, the network bandwidth between your iSCSI initiator and gateway VM, the amount of local storage allocated to the gateway VM, and the bandwidth between the gateway VM and Amazon S3.</p>
<p>AWS Storage Gateway efficiently uses your Internet bandwidth to speed up the upload of your on-premises application data to AWS. AWS Storage Gateway only uploads data that has changed, which minimizes the amount of data sent over the Internet. To further increase throughput and reduce your network costs, you can also use AWS Direct Connect to establish a dedicated network connection between your on-premises gateway and AWS.</p>
<h5 id="durability-and-availability-5">Durability and Availability</h5>
<p>AWS Storage Gateway durably stores your on-premises application data by uploading it to Amazon S3 or Amazon Glacier. Both of these AWS services store data in multiple facilities and on multiple devices within each facility, being designed to provide an average annual durability of 99.999999999 percent (11 nines). They also perform regular, systematic data integrity checks and are built to be automatically self-healing.</p>
<h5 id="scalability-and-elasticity-4">Scalability and Elasticity</h5>
<p>In both gateway-cached and gateway-stored volume configurations, AWS Storage Gateway stores data in Amazon S3, which has been designed to offer a very high level of scalability and elasticity automatically. Unlike a typical file system that can encounter issues when storing large number of files in a directory, Amazon S3 supports a virtually unlimited number of files in any bucket.</p>
<p>In a gateway-VTL configuration, AWS Storage Gateway stores data in Amazon S3 or Amazon Glacier, providing a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p>
<h5 id="security-5">Security</h5>
<p>IAM helps you provide security in controlling access to AWS Storage Gateway. With IAM, you can create multiple IAM users under your AWS account. The AWS Storage Gateway API enables a list of actions each IAM user can perform on AWS Storage Gateway.</p>
<p>The AWS Storage Gateway encrypts all data in transit to and from AWS by using SSL. All volume and snapshot data stored in AWS using gateway-stored or gateway-cached volumes and all virtual tape data stored in AWS using a gatewayVTL is encrypted at rest using AES-256, a secure symmetric-key encryption standard using 256-bit encryption keys. Storage Gateway supports authentication between your gateway and iSCSI iniitiators by using Challenge-Handshake Authentication Protocol (CHAP).</p>
<h5 id="interfaces-5">Interfaces</h5>
<p>AWS Storage Gateway provides:</p>
<ol>
<li>AWS Management Console.</li>
<li>Integrated AWS CLI provides a set of high-level, Linux-like commands for common operations of the AWS Storage Gateway service.</li>
<li>AWS SDKs to develop applications that interact with AWS Storage Gateway</li>
</ol>
<h5 id="cost-model-5">Cost Model</h5>
<p>With AWS Storage Gateway, you pay only for what you use. AWS Storage Gateway has the following pricing components: gateway usage (per gateway per month), snapshot storage usage (per GB per month), volume storage usage (per GB per month), virtual tape shelf storage (per GB per month), virtual tape library storage (per GB per month), retrieval from virtual tape shelf (per GB), and data transfer out (per GB per month).</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="aws-snowball">AWS Snowball</h3>
<p>AWS Snowball accelerates moving large amounts of data into and out of AWS using secure Snowball appliances.51 The Snowball appliance is purpose-built for efficient data storage and transfer. All AWS Regions have 80 TB Snowballs while US Regions have both 50 TB and 80 TB models</p>
<p>AWS transfers your data directly onto and off of Snowball storage devices using Amazon’s high-speed internal network and bypasses the Internet. For datasets of significant size, Snowball is often faster than Internet transfer and more cost effective than upgrading your connectivity. AWS Snowball supports importing data into and exporting data from Amazon S3 buckets.</p>
<h5 id="usage-patterns-4">Usage Patterns</h5>
<p>Snowball is ideal for transferring anywhere from terabytes to many petabytes of data in and out of the AWS Cloud securely. This is especially beneficial in cases where you don’t want to make expensive upgrades to your network infrastructure or in areas where high-speed Internet connections are not available or costprohibitive.</p>
<p>Common use cases include cloud migration, disaster recovery, data center decommission, and content distribution. When you decommission a data center, many steps are involved to make sure valuable data is not lost, and Snowball can help ensure data is securely and cost-effectively transferred to AWS.</p>
<h5 id="performance-5">Performance</h5>
<p>The Snowball appliance is purpose-built for efficient data storage and transfer, including a high-speed, 10 Gbps network connection designed to minimize data transfer times, allowing you to transfer up to 80 TB of data from your data source to the appliance in 2.5 days, plus shipping time. In this case, the end-to-end time to transfer the data into AWS is approximately a week, including default shipping and handling time to AWS data centers. Copying 160 TB of data can be completed in the same amount of time by using two 80 TB Snowballs in parallel.</p>
<h5 id="durability-and-availability-6">Durability and Availability</h5>
<p>Once the data is imported to AWS, the durability and availability characteristics of the target storage applies. Amazon S3 is designed for 99.999999999 percent (11 nines) durability and 99.99 percent availability.</p>
<h5 id="scalability-and-elasticity-5">Scalability and Elasticity</h5>
<p>Each AWS Snowball appliance is capable of storing 50 TB or 80 TB of data. If you want to transfer more data than that, you can use multiple appliances. For Amazon S3, individual files are loaded as objects and can range up to 5 TB in size, but you can load any number of objects in Amazon S3. The aggregate total amount of data that can be imported is virtually unlimited.</p>
<h5 id="security-6">Security</h5>
<p>You can integrate Snowball with IAM to control which actions a user can perform. You can give the IAM users on your AWS account access to all Snowball actions or to a subset of them. Similarly, an IAM user that creates a Snowball job must have permissions to access the Amazon S3 buckets that will be used for the import operations.</p>
<p>For Snowball, AWS KMS protects the encryption keys used to protect data on each Snowball appliance. All data loaded onto a Snowball appliance is encrypted using 256-bit encryption.</p>
<p>Snowball is physically secured by using an industry- standard Trusted Platform Module (TPM) that uses a dedicated processor designed to detect any unauthorized modifications to the hardware, firmware, or software.</p>
<p>Snowball is included in the AWS HIPAA compliance program so you can use Snowball to transfer large amounts of Protected Health Information (PHI) data into and out of AWS.</p>
<h5 id="interfaces-6">Interfaces</h5>
<p>There are two ways to get started with Snowball. You can create an import or export job using the AWS Snowball Management Console or you can use the Snowball Job Management API and integrate AWS Snowball as a part of your data management solution. The primary functions of the API are to create, list, and describe import and export jobs, and it uses a simple standards-based REST web services interface. For more details around using the Snowball Job Management API, see the API Reference documentation.</p>
<p>You also have two ways to locally transfer data between a Snowball appliance and your on-premises data center. The Snowball client, available as a download from the AWS Import/Export Tools page, is a standalone terminal application that you run on your local workstation to do your data transfer.56 You use simple copy (cp) commands to transfer data, and handling errors and logs are written to your local workstation for troubleshooting and auditing. The second option to locally transfer data between a Snowball appliance and your on-premises data center is the Amazon S3 Adapter for Snowball, which is also available as a download from the AWS Import/Export Tools page. You can programmatically transfer data between your on-premises data center and a Snowball appliance using a subset of the Amazon S3 REST API commands. This allows you to have direct access to a Snowball appliance as if it were an Amazon S3 endpoint.</p>
<h5 id="cost-model-6">Cost Model</h5>
<p>With Snowball, as with most other AWS services, you pay only for what you use. Snowball has three pricing components: service fee (per job), extra day charges as required (the first 10 days of onsite usage are free), and data transfer. For the destination storage, the standard Amazon S3 storage pricing applies.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>
<h3 id="amazon-cloudfront">Amazon CloudFront</h3>
<p>Amazon CloudFront is a content-delivery web service that speeds up the distribution of your website’s dynamic, static, and streaming content by making it available from a global network of edge locations.58 When a user requests content that you’re serving with Amazon CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so content is delivered with better performance than if the user had accessed the content from a data center farther away. If the content is already in the edge location with the lowest latency, Amazon CloudFront delivers it immediately. If the content is not currently in that edge location, Amazon CloudFront retrieves it from an Amazon S3 bucket or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content. Amazon CloudFront caches content at edge locations for a period of time that you specify.</p>
<p>Amazon CloudFront supports all files that can be served over HTTP. These files include dynamic web pages, such as HTML or PHP pages, and any popular static files that are a part of your web application, such as website images, audio, video, media files or software downloads. For on-demand media files, you can also choose to stream your content using Real-Time Messaging Protocol (RTMP) delivery. Amazon CloudFront also supports delivery of live media over HTTP.</p>
<p>Amazon CloudFront is optimized to work with other Amazon web services, such as Amazon S3, Amazon EC2, Elastic Load Balancing, and Amazon Route 53.</p>
<p>Amazon CloudFront also works seamlessly with any non-AWS origin servers that store the original, definitive versions of your files.</p>
<h5 id="usage-patterns-5">Usage Patterns</h5>
<p>CloudFront is ideal for distribution of frequently accessed static content that benefits from edge delivery, such as popular website images, videos, media files or software downloads. Amazon CloudFront can also be used to deliver dynamic web applications over HTTP. These applications can include static content, dynamic content, or a whole site with a mixture of the two. Amazon CloudFront is also commonly used to stream audio and video files to web browsers and mobile devices.</p>
<p>If you need to remove an object from Amazon CloudFront edge-server caches before it expires, you can either invalidate the object or use object versioning to serve a different version of the object that has a different name.</p>
<h5 id="performance-6">Performance</h5>
<p>Amazon CloudFront is designed for low-latency and high-bandwidth delivery of content. Amazon CloudFront speeds up the distribution of your content by routing end users to the edge location that can best serve each end user’s request in a worldwide network of edge locations. Typically, requests are routed to the nearest Amazon CloudFront edge location in terms of latency.</p>
<h5 id="durability-and-availability-7">Durability and Availability</h5>
<p>Because a CDN is an edge cache, Amazon CloudFront does not provide durable storage. The origin server, such as Amazon S3 or a web server running on Amazon EC2, provides the durable file storage needed. Amazon CloudFront provides high availability by using a distributed global network of edge locations. Origin requests from the edge locations to AWS origin servers (for example, Amazon EC2, Amazon S3, and so on) are carried over network paths that Amazon constantly monitors and optimizes for both availability and performance. This edge network provides increased reliability and availability because there is no longer a central point of failure. Copies of your files are now held in edge locations around the world.</p>
<h5 id="scalability-and-elasticity-6">Scalability and Elasticity</h5>
<p>Amazon CloudFront uses multiple layers of caching at each edge location and collapses simultaneous requests for the same object before contacting your origin server. These optimizations further reduce the need to scale your origin infrastructure as your website becomes more popular.</p>
<h5 id="security-7">Security</h5>
<p>Amazon CloudFront is a very secure service to distribute your data. It integrates with IAM so that you can create users for your AWS account and specify which Amazon CloudFront actions a user (or a group of users) can perform in your AWS account.</p>
<p>You can configure Amazon CloudFront to create log files that contain detailed information about every user request that Amazon CloudFront receives. These access logs are available for both web and RTMP distributions.62 Additionally, Amazon CloudFront integrates with Amazon CloudWatch metrics so that you can monitor your website or application.</p>
<h5 id="interfaces-7">Interfaces</h5>
<p>You can manage and configure Amazon CloudFront in several ways. The AWS Management Console provides an easy way to manage Amazon CloudFront and supports all features of the Amazon CloudFront API. For example, you can enable or disable distributions, configure CNAMEs, and enable end-user logging using the console. You can also use the Amazon CloudFront command line tools, the native REST API, or one of the supported SDKs.</p>
<p>There is no data API for Amazon CloudFront and no command to preload data. Instead, data is automatically pulled into Amazon CloudFront edge locations on the first access of an object from that location.</p>
<p>Clients access content from CloudFront edge locations either using HTTP or HTTPs from locations across the Internet; these protocols are configurable as part of a given CloudFront distribution.</p>
<h5 id="cost-model-7">Cost Model</h5>
<p>With Amazon CloudFront, there are no long-term contracts or required minimum monthly commitments—you pay only for as much content as you actually deliver through the service. Amazon CloudFront has two pricing components: regional data transfer out (per GB) and requests (per 10,000). As part of the Free Usage Tier, new AWS customers don’t get charged for 50 GB data transfer out and 2,000,000 HTTP and HTTPS requests each month for one year. Note that if you use an AWS service as the origin (for example, Amazon S3, Amazon EC2, Elastic Load Balancing, or others), data transferred from the origin to edge locations (i.e., Amazon CloudFront “origin fetches”) will be free of charge. For web distributions, data transfer out of Amazon CloudFront to your origin server will be billed at the “Regional Data Transfer Out of Origin” rates.</p>
<p><a href="https://adrian83.github.io/page/aws_architect">Back to main page</a></p>

  </article>
</section>


      </div>

      <footer class="footer">
  <section class="container">
    
    
      
        © 2020
      
       Adrian Brzoza 
    
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>

    </main>

    

    

  </body>

</html>
